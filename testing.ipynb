{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26f6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337ecb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b12890",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = ChatPromptTemplate.from_messages([SystemMessage(content=f\"\"\"\n",
    "            You are a Tavily Search Query specialist. Follow the JSON schema below exactly:\n",
    "\n",
    "            Rules:\n",
    "            1. Generate Tavily DSL only (no natural language outside the JSON).\n",
    "            2. Map the job description into five categories:\n",
    "            • query1: recent developments\n",
    "            • query2: recent news\n",
    "            • query3:company profile\n",
    "            • query4: key customers & partners\n",
    "            • query5: culture & values\n",
    "            3. Each value is a two‑element list:\n",
    "            [<query string>, <one‑sentence rationale>]\n",
    "            4. Use filters (source:, date:[now-30d TO now], site:…, etc.) where helpful.\n",
    "            5. If information is missing in the JD, fall back sensibly\n",
    "            (e.g. search for “employee testimonials”).\n",
    "            6. Return **only** valid JSON.\n",
    "        \"\"\"\n",
    "    )\n",
    "    , HumanMessage(content=\"Hello World\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38c3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = ChatPromptTemplate.from_messages([HumanMessage(content=\"Hello World\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac1ec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "            You are a Tavily Search Query specialist. Follow the JSON schema below exactly:\n",
      "\n",
      "            Rules:\n",
      "            1. Generate Tavily DSL only (no natural language outside the JSON).\n",
      "            2. Map the job description into five categories:\n",
      "            • query1: recent developments\n",
      "            • query2: recent news\n",
      "            • query3:company profile\n",
      "            • query4: key customers & partners\n",
      "            • query5: culture & values\n",
      "            3. Each value is a two‑element list:\n",
      "            [<query string>, <one‑sentence rationale>]\n",
      "            4. Use filters (source:, date:[now-30d TO now], site:…, etc.) where helpful.\n",
      "            5. If information is missing in the JD, fall back sensibly\n",
      "            (e.g. search for “employee testimonials”).\n",
      "            6. Return **only** valid JSON.\n",
      "        \n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "messages.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ebd0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "input_message = HumanMessagePromptTemplate.from_template(\"Below is the required job description and resume: {background_information}\", input_variables=[\"background_information\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6b3cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='Below is the required job description and resume: This is Rishabh', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_message.format(background_information=\"This is Rishabh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9628bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c352da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collapse_ws(text: str) -> str:\n",
    "    \"\"\"Collapse stray whitespace but keep bullet breaks.\"\"\"\n",
    "    text = re.sub(r\"\\n\\s*([•\\-–])\\s*\", r\"\\n\\1 \", text)\n",
    "    return re.sub(r\"[ \\t\\r\\f\\v]+\", \" \", text).replace(\" \\n\", \"\\n\").strip()\n",
    "\n",
    "\n",
    "def _is_heading(line: str) -> bool:\n",
    "    return (\n",
    "        line.isupper()\n",
    "        and len(line.split()) <= 5\n",
    "        and not re.search(r\"\\d\", line)\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_resume(pdf_path: str | Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a single‑page résumé PDF → list[Document] chunks\n",
    "    (≈400 chars, 50‑char overlap) with {source, section} metadata.\n",
    "    \"\"\"\n",
    "    text = PyPDFLoader(str(pdf_path), extraction_mode=\"layout\").load()[0].page_content\n",
    "    print(text)\n",
    "    text = _collapse_ws(text)\n",
    "\n",
    "    # Tag headings with \"###\" so Markdown splitter can see them\n",
    "    tagged_lines = [\n",
    "        f\"### {ln}\" if _is_heading(ln) else ln\n",
    "        for ln in text.splitlines()\n",
    "    ]\n",
    "    md_text = \"\\n\".join(tagged_lines)\n",
    "\n",
    "    if \"###\" in md_text:\n",
    "        splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=[(\"###\", \"section\")]\n",
    "        )\n",
    "        chunks = splitter.split_text(md_text)  # already returns Documents\n",
    "    else:\n",
    "        print(f\"No headings found.\")\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400, chunk_overlap=50\n",
    "        )\n",
    "        chunks = [\n",
    "            Document(page_content=chunk, metadata={})\n",
    "            for chunk in splitter.split_text(md_text)\n",
    "        ]\n",
    "\n",
    "    # Attach metadata\n",
    "    for doc in chunks:\n",
    "        doc.metadata.setdefault(\"source\", str(pdf_path))\n",
    "        # section already present if header‑splitter was used\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14e062e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rishabh Aggarwal\n",
      "                                      (602) 580-5734  •      raggar15@asu.edu  •       LinkedIn  •    Tempe, AZ\n",
      "TECHNICAL       SKILLS\n",
      "Programming Languages: Python, Java, JavaScript, Bash, HTML, CSS\n",
      "Databases: SQL (PostgreSQL, MySQL, SQLite), NoSQL (MongoDB,  Redis, DynamoDB, Pinecone)\n",
      "Frameworks/Tools: SpringBoot, React, JUnit, Node.js, RESTful APIs, Django, Kafka, Airﬂow, FastAPI, Pydantic, Tableau\n",
      "DevOps/Cloud: AWS, GCP, GitHub Actions, Docker, Jenkins, Terraform, Kubernetes, MLFlow, GitLab\n",
      "AI Tools/Frameworks: PyTorch, Tensorﬂow, scikit-learn, LangGraph, LangChain, LangSmith, ChatGPT\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Amazon Inc, Tempe, AZ: Software Development Engineer | Seller Payment Services                                                Dec 2023 - Aug 2024\n",
      "●   Established AWS Evidently        setup to handle 50K+ daily API requests           to new Lambda service using AWS CDK(TypeScript)\n",
      "●   Added metrics      to monitor traﬃc and enhance service observability of the Lambda service through CloudWatch logs\n",
      "●   Developed SNS Event Publishers           in Java using Spring Boot to process          10K+ daily events in an event-driven architecture\n",
      "●   Led load balancer migration planning for a microservice with a focus                   on safe rollbacks    and minimum downtime\n",
      "●   Designed a dashboard for ALB migration to monitor traﬃc with high-severity                  alarms   to enhance observability\n",
      "●   Directed weekly     meetings    with a 7-member agile team to analyze metrics            and customer data, guiding decision-making for\n",
      "    live campaigns     involving over 50K sellers\n",
      "MetaJungle, Ozark, MO: Lead Backend Engineer                                                                                   Jun 2023 - Dec 2023\n",
      "●   Architected    a  scalable    AWS     cloud    infrastructure    for  a  Marketplace     using   Terraform     IaC   with   ECS   and   Fargate\n",
      "    instances, reduced costs       by  40% while maintaining high reliability      using Blue/Green deployment strategy\n",
      "●   Engineered and managed Jenkins CI/CD pipeline allowing faster iterative development by                       reducing deployment time by\n",
      "    75% ,  leveraging Github hooks        and Docker Containerization\n",
      "●   Migrated over     1.2TB   on-premises     Microsoft     SQL Server database          with  over 2 million records     to AWS RDS,       utilizing\n",
      "    AWS DMS ensuring eﬃcient indexing and retrieval\n",
      "●   Developed 10+ RESTful APIs in Node.js to manage data for over 500 NFT collections                       and 10,000 listings    from MongoDB\n",
      "●   Automated extraction and compression of 50,000+                 images     from Ethereum Blockchain and stored on AWS S3 using\n",
      "    Airﬂow workﬂows in Python, leading to almost 30%                  storage cost savings\n",
      "Omnipresent Robot Technologies, Delhi, India: Software Engineer                                                              Jun 2018 - Jul 2021\n",
      "●     Engineered a distributed, scalable AI           surveillance   application with edge-device computation using Python, OpenCV,\n",
      "      and scikit-learn, ensuring security       for 10,000+ daily park visitors\n",
      "●     Architected a distributed system for real-time video streaming using Apache Kafka and Python to process                          50+ parallel\n",
      "      video streams, reducing latency by 60%            by  rigorous   debugging and performance optimization\n",
      "●     Led the development of an analytics          dashboard using Django, React and Postgres to show breach records, alerts, and\n",
      "      intuitive data visualizations   using Google Charts, allowing data-driven decision making\n",
      "●     Developed     a drone   compliance     platform using Django to automate ﬂight authorization and authentication process,\n",
      "      leading to enhanced productivity        of the drone engineering team\n",
      "●     Led collaboration of a team of engineers         and drone operators      to conduct real-world testing of the compliance system\n",
      "●     Mentored interns     to understand software development best practices, coding standards, and version control systems\n",
      "ADDITIONAL EXPERIENCE\n",
      "ML Software Developer at ASU                                                                                                Jul 2022 - May 2023\n",
      "●   Trained deep learning models        using PyTorch and Scikit to detect low-resolution objects            in 15,000+ satellite images\n",
      "●   Executed     adversarial   attacks    and   utilized  MLFlow     for ﬁne-tuning     multi-class   classiﬁcation    machine     learning  model,\n",
      "    enhancing model robustness          and improving accuracy       by  20%\n",
      "Mayhem Heroes Cybersecurity Open Source Hackathon                                                                                           Apr 2022\n",
      "Integrated Mayhem into CI/CD pipeline for Open Source repos                using GitHub Actions, reducing security          risks  by over 80%\n",
      " EDUCATION\n",
      "Master of Science in Information Technology\n",
      "Arizona State University, Tempe, Arizona\n"
     ]
    }
   ],
   "source": [
    "chunks = parse_resume(\"C:\\\\Users\\\\risha\\\\Downloads\\\\Rishabh_SDE_Resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0100cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume chunk: Rishabh Aggarwal\n",
      "(602) 580-5734 • raggar15@asu.edu • LinkedIn • Tempe, AZ\n",
      "Resume chunk: Programming Languages: Python, Java, JavaScript, Bash, HTML, CSS\n",
      "Databases: SQL (PostgreSQL, MySQL, SQLite), NoSQL (MongoDB, Redis, DynamoDB, Pinecone)\n",
      "Frameworks/Tools: SpringBoot, React, JUnit, Node.js, RESTful APIs, Django, Kafka, Airﬂow, FastAPI, Pydantic, Tableau\n",
      "DevOps/Cloud: AWS, GCP, GitHub Actions, Docker, Jenkins, Terraform, Kubernetes, MLFlow, GitLab\n",
      "AI Tools/Frameworks: PyTorch, Tensorﬂow, scikit-learn, LangGraph, LangChain, LangSmith, ChatGPT\n",
      "Resume chunk: Amazon Inc, Tempe, AZ: Software Development Engineer | Seller Payment Services Dec 2023 - Aug 2024\n",
      "● Established AWS Evidently setup to handle 50K+ daily API requests to new Lambda service using AWS CDK(TypeScript)\n",
      "● Added metrics to monitor traﬃc and enhance service observability of the Lambda service through CloudWatch logs\n",
      "● Developed SNS Event Publishers in Java using Spring Boot to process 10K+ daily events in an event-driven architecture\n",
      "● Led load balancer migration planning for a microservice with a focus on safe rollbacks and minimum downtime\n",
      "● Designed a dashboard for ALB migration to monitor traﬃc with high-severity alarms to enhance observability\n",
      "● Directed weekly meetings with a 7-member agile team to analyze metrics and customer data, guiding decision-making for\n",
      "live campaigns involving over 50K sellers\n",
      "MetaJungle, Ozark, MO: Lead Backend Engineer Jun 2023 - Dec 2023\n",
      "● Architected a scalable AWS cloud infrastructure for a Marketplace using Terraform IaC with ECS and Fargate\n",
      "instances, reduced costs by 40% while maintaining high reliability using Blue/Green deployment strategy\n",
      "● Engineered and managed Jenkins CI/CD pipeline allowing faster iterative development by reducing deployment time by\n",
      "75% , leveraging Github hooks and Docker Containerization\n",
      "● Migrated over 1.2TB on-premises Microsoft SQL Server database with over 2 million records to AWS RDS, utilizing\n",
      "AWS DMS ensuring eﬃcient indexing and retrieval\n",
      "● Developed 10+ RESTful APIs in Node.js to manage data for over 500 NFT collections and 10,000 listings from MongoDB\n",
      "● Automated extraction and compression of 50,000+ images from Ethereum Blockchain and stored on AWS S3 using\n",
      "Airﬂow workﬂows in Python, leading to almost 30% storage cost savings\n",
      "Omnipresent Robot Technologies, Delhi, India: Software Engineer Jun 2018 - Jul 2021\n",
      "● Engineered a distributed, scalable AI surveillance application with edge-device computation using Python, OpenCV,\n",
      "and scikit-learn, ensuring security for 10,000+ daily park visitors\n",
      "● Architected a distributed system for real-time video streaming using Apache Kafka and Python to process 50+ parallel\n",
      "video streams, reducing latency by 60% by rigorous debugging and performance optimization\n",
      "● Led the development of an analytics dashboard using Django, React and Postgres to show breach records, alerts, and\n",
      "intuitive data visualizations using Google Charts, allowing data-driven decision making\n",
      "● Developed a drone compliance platform using Django to automate ﬂight authorization and authentication process,\n",
      "leading to enhanced productivity of the drone engineering team\n",
      "● Led collaboration of a team of engineers and drone operators to conduct real-world testing of the compliance system\n",
      "● Mentored interns to understand software development best practices, coding standards, and version control systems\n",
      "Resume chunk: ML Software Developer at ASU Jul 2022 - May 2023\n",
      "● Trained deep learning models using PyTorch and Scikit to detect low-resolution objects in 15,000+ satellite images\n",
      "● Executed adversarial attacks and utilized MLFlow for ﬁne-tuning multi-class classiﬁcation machine learning model,\n",
      "enhancing model robustness and improving accuracy by 20%\n",
      "Mayhem Heroes Cybersecurity Open Source Hackathon Apr 2022\n",
      "Integrated Mayhem into CI/CD pipeline for Open Source repos using GitHub Actions, reducing security risks by over 80%\n",
      "Resume chunk: Master of Science in Information Technology\n",
      "Arizona State University, Tempe, Arizona\n"
     ]
    }
   ],
   "source": [
    "resume_text = \"\"\n",
    "for chunk in chunks:\n",
    "  print(f\"Resume chunk: {chunk.page_content}\")\n",
    "  resume_text+= (chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b045de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TavilyQuerySet(BaseModel):\n",
    "    query1: tuple[str, str] = Field(\n",
    "        ...,\n",
    "        description=\"DSL for Recent Developments + 1‑sentence rationale\",\n",
    "    )\n",
    "    query2: tuple[str, str] = Field(\n",
    "        ...,\n",
    "        description=\"DSL for Recent News + rationale\",\n",
    "    )\n",
    "    query3: tuple[str, str]\n",
    "    query4: tuple[str, str]\n",
    "    query5: tuple[str, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eda95e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=TavilyQuerySet)\n",
    "\n",
    "messages = SystemMessage(content=f\"\"\"\n",
    "            You are a Tavily Search Query specialist. Follow the JSON schema below exactly:\n",
    "            {parser.get_format_instructions()}\n",
    "\n",
    "           \n",
    "            Rules:\n",
    "            1. Generate Tavily DSL only (no natural language outside the JSON).\n",
    "            2. Map the job description into five categories:\n",
    "            • query1: recent developments\n",
    "            • query2: recent news\n",
    "            • query3:company profile\n",
    "            • query4: key customers & partners\n",
    "            • query5: culture & values\n",
    "            3. Each value is a two‑element list:\n",
    "            [<query string>, <one‑sentence rationale>]\n",
    "            4. Use filters (source:, date:[now-30d TO now], site:…, etc.) where helpful.\n",
    "            5. If information is missing in the JD, fall back sensibly\n",
    "            (e.g. search for “employee testimonials”).\n",
    "            6. Return **only** valid JSON.\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9738103e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"query1\": {\"description\": \"DSL for Recent Developments + 1‑sentence rationale\", \"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query1\", \"type\": \"array\"}, \"query2\": {\"description\": \"DSL for Recent News + rationale\", \"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query2\", \"type\": \"array\"}, \"query3\": {\"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query3\", \"type\": \"array\"}, \"query4\": {\"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query4\", \"type\": \"array\"}, \"query5\": {\"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query5\", \"type\": \"array\"}}, \"required\": [\"query1\", \"query2\", \"query3\", \"query4\", \"query5\"]}\\n```'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3174432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'query1': {'description': 'DSL for Recent Developments + 1‑sentence rationale',\n",
       "   'maxItems': 2,\n",
       "   'minItems': 2,\n",
       "   'prefixItems': [{'type': 'string'}, {'type': 'string'}],\n",
       "   'title': 'Query1',\n",
       "   'type': 'array'},\n",
       "  'query2': {'description': 'DSL for Recent News + rationale',\n",
       "   'maxItems': 2,\n",
       "   'minItems': 2,\n",
       "   'prefixItems': [{'type': 'string'}, {'type': 'string'}],\n",
       "   'title': 'Query2',\n",
       "   'type': 'array'},\n",
       "  'query3': {'maxItems': 2,\n",
       "   'minItems': 2,\n",
       "   'prefixItems': [{'type': 'string'}, {'type': 'string'}],\n",
       "   'title': 'Query3',\n",
       "   'type': 'array'},\n",
       "  'query4': {'maxItems': 2,\n",
       "   'minItems': 2,\n",
       "   'prefixItems': [{'type': 'string'}, {'type': 'string'}],\n",
       "   'title': 'Query4',\n",
       "   'type': 'array'},\n",
       "  'query5': {'maxItems': 2,\n",
       "   'minItems': 2,\n",
       "   'prefixItems': [{'type': 'string'}, {'type': 'string'}],\n",
       "   'title': 'Query5',\n",
       "   'type': 'array'}},\n",
       " 'required': ['query1', 'query2', 'query3', 'query4', 'query5'],\n",
       " 'title': 'TavilyQuerySet',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TavilyQuerySet.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5884df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "            You are a Tavily Search Query specialist. Follow the JSON schema below exactly:\n",
      "            The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"query1\": {\"description\": \"DSL for Recent Developments + 1‑sentence rationale\", \"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query1\", \"type\": \"array\"}, \"query2\": {\"description\": \"DSL for Recent News + rationale\", \"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query2\", \"type\": \"array\"}, \"query3\": {\"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query3\", \"type\": \"array\"}, \"query4\": {\"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query4\", \"type\": \"array\"}, \"query5\": {\"maxItems\": 2, \"minItems\": 2, \"prefixItems\": [{\"type\": \"string\"}, {\"type\": \"string\"}], \"title\": \"Query5\", \"type\": \"array\"}}, \"required\": [\"query1\", \"query2\", \"query3\", \"query4\", \"query5\"]}\n",
      "```\n",
      "\n",
      "\n",
      "            Rules:\n",
      "            1. Generate Tavily DSL only (no natural language outside the JSON).\n",
      "            2. Map the job description into five categories:\n",
      "            • query1: recent developments\n",
      "            • query2: recent news\n",
      "            • query3:company profile\n",
      "            • query4: key customers & partners\n",
      "            • query5: culture & values\n",
      "            3. Each value is a two‑element list:\n",
      "            [<query string>, <one‑sentence rationale>]\n",
      "            4. Use filters (source:, date:[now-30d TO now], site:…, etc.) where helpful.\n",
      "            5. If information is missing in the JD, fall back sensibly\n",
      "            (e.g. search for “employee testimonials”).\n",
      "            6. Return **only** valid JSON.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "messages.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2c3cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"\"\"properties\": {\"query1\": [{\"query\": \"Shalin Mehta AND \\\"Computational Microscopy Platform\\\"\", \"rationale\": \"Recent developments within the company\"}, {\"query\": \"Shalin Mehta AND \\\"Biohub SF\\\"\", \"rationale\": \"Recent developments within the company\"}], \"query2\": [{\"query\": \"Chan Zuckerberg Biohub - San Francisco AND recent news\", \"rationale\": \"Recent news about the company\"}, {\"query\": \"COVID-19 AND Chan Zuckerberg Biohub - San Francisco\", \"rationale\": \"Recent news about the company\"}], \"query3\": [{\"query\": \"Shalin Mehta AND \\\"role: Software Engineer\\\"\", \"rationale\": \"Information about the company that relates to the role\"}, {\"query\": \"Chan Zuckerberg Biohub - San Francisco AND \\\"team: Bioengineering\\\"\", \"rationale\": \"Information about the company that relates to the role\"}], \"query4\": [{\"query\": \"key customers: Chan Zuckerberg Biohub\", \"rationale\": \"Key customers & partners\"}, {\"query\": \"partners: Chan Zuckerberg Biohub SF\", \"rationale\": \"Key customers & partners\"}], \"query5\": [{\"query\": \"company culture: Chan Zuckerberg Biohub\", \"rationale\": \"Culture & values of the company\"}, {\"query\": \"values: Chan Zuckerberg Biohub\", \"rationale\": \"Culture & values of the company\"}]}, \"required\": [\"query1\", \"query2\", \"query3\", \"query4\", \"query5\"]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d8508a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties\": {\"query1\": [{\"query\": \"Shalin Mehta AND \"Computational Microscopy Platform\"\", \"rationale\": \"Recent developments within the company\"}, {\"query\": \"Shalin Mehta AND \"Biohub SF\"\", \"rationale\": \"Recent developments within the company\"}], \"query2\": [{\"query\": \"Chan Zuckerberg Biohub - San Francisco AND recent news\", \"rationale\": \"Recent news about the company\"}, {\"query\": \"COVID-19 AND Chan Zuckerberg Biohub - San Francisco\", \"rationale\": \"Recent news about the company\"}], \"query3\": [{\"query\": \"Shalin Mehta AND \"role: Software Engineer\"\", \"rationale\": \"Information about the company that relates to the role\"}, {\"query\": \"Chan Zuckerberg Biohub - San Francisco AND \"team: Bioengineering\"\", \"rationale\": \"Information about the company that relates to the role\"}], \"query4\": [{\"query\": \"key customers: Chan Zuckerberg Biohub\", \"rationale\": \"Key customers & partners\"}, {\"query\": \"partners: Chan Zuckerberg Biohub SF\", \"rationale\": \"Key customers & partners\"}], \"query5\": [{\"query\": \"company culture: Chan Zuckerberg Biohub\", \"rationale\": \"Culture & values of the company\"}, {\"query\": \"values: Chan Zuckerberg Biohub\", \"rationale\": \"Culture & values of the company\"}]}, \"required\": [\"query1\", \"query2\", \"query3\", \"query4\", \"query5\"]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fab5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93695ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Below is the required job description and resume: {background_information}\", input_variables=[\"background_information\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5330010",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ('query1', ('recent developments within the company', 'The Associate Software engineer will build open source software tools for managing and processing 10-100 terabyte-scale datasets.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5753afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ('q', ('y', 'z'))\n",
    "\n",
    "dict_x = dict(zip(keys, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06d50119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('recent developments within the company',\n",
       " 'The Associate Software engineer will build open source software tools for managing and processing 10-100 terabyte-scale datasets.')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_x[('y', 'z')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser, RetryOutputParser\n",
    "base_parser = PydanticOutputParser(pydantic_object=TavilyQuerySet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dd9c74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m format_instructions = \u001b[43mparser\u001b[49m.get_format_instructions()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chat\n\u001b[32m      5\u001b[39m tavily_role_messages = SystemMessage(content=\n\u001b[32m      6\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m            When you reply, output **only** valid JSON that can be parsed\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m \u001b[33m            5. Return **only** valid JSON that matches the schema exactly. No other fields\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33m            \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "format_instructions = parser.get_format_instructions()\n",
    "from ollama import chat\n",
    "\n",
    "\n",
    "tavily_role_messages = SystemMessage(content=\n",
    "            f\"\"\"\n",
    "            When you reply, output **only** valid JSON that can be parsed\n",
    "            into the Pydantic model shown below. Do **not** wrap it in \"properties\"\n",
    "            or \"required\".:\n",
    "            \n",
    "            ------------------------------------------------\n",
    "\n",
    "\n",
    "            {format_instructions}\n",
    "\n",
    "            \n",
    "           -------------------------------------------------\n",
    "\n",
    "            Rules:\n",
    "            1. Generate Tavily DSL only (no natural language outside the JSON).\n",
    "            2. Map the job description into five categories:\n",
    "            • query1: recent developments within the company\n",
    "            • query2: recent news about the company\n",
    "            • query3: information about the company that relates to the role\n",
    "            • query4: key customers & partners\n",
    "            • query5: culture & values of the company\n",
    "            3. Each value is a two‑element list:\n",
    "            [<query string>, <one‑sentence rationale>]\n",
    "            4. If information is missing in the JD, fall back sensibly\n",
    "            (e.g. search for “employee testimonials”).\n",
    "            5. Return **only** valid JSON that matches the schema exactly. No other fields\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "response = chat(\n",
    "    messages=[{\n",
    "        tavily_role_messages,\n",
    "        input_message}\n",
    "      ],\n",
    "    model='llama3.2:latest',\n",
    "    format=TavilyQuerySet.model_json_schema(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8deb0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ('query1', ['Recent developments within the company using computational microscopy platform', 'This project will require working on microscopes in a BSL-2 imaging laboratory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fcab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recent developments within the company using computational microscopy platform'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e3f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COVER_LETTER_PROMPT = SystemMessage(content=\"\"\"You are my dedicated assistant for writing job application content, including cover letters, LinkedIn outreach messages, and responses to job-specific questions (e.g., experience, culture fit, or motivation).\n",
    "\n",
    "Your goal is to generate content that:\n",
    "1. Reflects **my personality**, tone, and authentic voice, based on examples I provide.\n",
    "2. Matches **my knowledge, experience, and interests**, which I’ll also share or update as needed.\n",
    "3. Adopts **my writing style and energy** (e.g., grounded, confident, thoughtful—but not overly polished or generic).\n",
    "4. Embeds **genuine enthusiasm or alignment** with the company or role, without sounding performative.\n",
    "5. Avoids filler, clichés, or overused corporate phrases—keep it **authentic and specific**.\n",
    "6. Learns over time by asking me relevant clarifying questions when needed (e.g., change in tone, new experience, updates to goals).\n",
    "7. Balances job description alignment with personal storytelling, roughly in a 75:25 ratio.\n",
    "8. Keeps outputs **concise** and within any given word or character limits.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea061e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "FirstDraftGenerationPromptTemplate = ChatPromptTemplate.from_messages([COVER_LETTER_PROMPT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b96cbe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are my dedicated assistant for writing job application content, including cover letters, LinkedIn outreach messages, and responses to job-specific questions (e.g., experience, culture fit, or motivation).\\n\\nYour goal is to generate content that:\\n1. Reflects **my personality**, tone, and authentic voice, based on examples I provide.\\n2. Matches **my knowledge, experience, and interests**, which I’ll also share or update as needed.\\n3. Adopts **my writing style and energy** (e.g., grounded, confident, thoughtful—but not overly polished or generic).\\n4. Embeds **genuine enthusiasm or alignment** with the company or role, without sounding performative.\\n5. Avoids filler, clichés, or overused corporate phrases—keep it **authentic and specific**.\\n6. Learns over time by asking me relevant clarifying questions when needed (e.g., change in tone, new experience, updates to goals).\\n7. Balances job description alignment with personal storytelling, roughly in a 75:25 ratio.\\n8. Keeps outputs **concise** and within any given word or character limits.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FirstDraftGenerationPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd03f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_application_session  = \"Heello World\"\n",
    "company_research_data = \"Company Research Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5fef665",
   "metadata": {},
   "outputs": [],
   "source": [
    "CurrentSessionContextMessage = HumanMessagePromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            # Resume and Job Description\n",
    "            {current_job_role}\n",
    "\n",
    "            # Company Information\n",
    "            {company_research_data}\n",
    "\n",
    "            Create a cover letter that highlights the match between my qualifications and the job requirements.\n",
    "            \"\"\",\n",
    "            input_variables=[\"current_job_role\",\n",
    "                        \"company_research_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c89ba644",
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstDraftGenerationPromptTemplate.append(CurrentSessionContextMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6997c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "            ({\"current_job_role\": lambda x: x[\"current_job_role\"],\n",
    "              \"company_research_data\": lambda x: x[\"company_research_data\"]})\n",
    "            | FirstDraftGenerationPromptTemplate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55f51dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  current_job_role: RunnableLambda(...),\n",
       "  company_research_data: RunnableLambda(...)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are my dedicated assistant for writing job application content, including cover letters, LinkedIn outreach messages, and responses to job-specific questions (e.g., experience, culture fit, or motivation).\\n\\nYour goal is to generate content that:\\n1. Reflects **my personality**, tone, and authentic voice, based on examples I provide.\\n2. Matches **my knowledge, experience, and interests**, which I’ll also share or update as needed.\\n3. Adopts **my writing style and energy** (e.g., grounded, confident, thoughtful—but not overly polished or generic).\\n4. Embeds **genuine enthusiasm or alignment** with the company or role, without sounding performative.\\n5. Avoids filler, clichés, or overused corporate phrases—keep it **authentic and specific**.\\n6. Learns over time by asking me relevant clarifying questions when needed (e.g., change in tone, new experience, updates to goals).\\n7. Balances job description alignment with personal storytelling, roughly in a 75:25 ratio.\\n8. Keeps outputs **concise** and within any given word or character limits.', additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['company_research_data', 'current_job_role'], input_types={}, partial_variables={}, template='\\n            # Resume and Job Description\\n            {current_job_role}\\n\\n            # Company Information\\n            {company_research_data}\\n\\n            Create a cover letter that highlights the match between my qualifications and the job requirements.\\n            '), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c54667",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMClient\n\u001b[32m      3\u001b[39m LLM = LLMClient()\n\u001b[32m      4\u001b[39m llm = LLMClient().get_llm()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from job_writer.utils.llm_client import LLMClient\n",
    "\n",
    "LLM = LLMClient()\n",
    "llm = LLMClient().get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421df9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_writer.tools.TavilySearch import search_company\n",
    "\n",
    "# Test job description\n",
    "test_job = \"\"\"\n",
    "Software Engineer - Backend\n",
    "OpenAI\n",
    "\n",
    "We are looking for experienced backend engineers to join our team. Our ideal candidate will have experience with one or more of the following technologies: Python, Java, C++. \n",
    "\n",
    "Responsibilities:\n",
    "- Design and implement scalable and efficient backend systems\n",
    "- Write clean, maintainable code\n",
    "- Work with cross-functional teams\n",
    "\n",
    "Requirements:\n",
    "- Strong proficiency in one or more programming languages\n",
    "- Strong understanding of software design patterns and principles\n",
    "- Experience with distributed systems\n",
    "\"\"\"\n",
    "\n",
    "# Test the search_company function\n",
    "results = search_company(test_job)\n",
    "for query_key, data in results.items():\n",
    "    print(f\"\\n{query_key}:\")\n",
    "    print(f\"Query: {data['query']}\")\n",
    "    print(f\"Rationale: {data['rationale']}\")\n",
    "    if data['results']:\n",
    "        print(f\"First result: {data['results'][0][:200]}...\")\n",
    "    else:\n",
    "        print(\"No results found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f12ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba77224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_writer.prompts.templates import (\n",
    "    TAVILY_QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bb7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_search_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=TAVILY_QUERY_PROMPT),\n",
    "    HumanMessage(\n",
    "        \"Below is the required job description and resume: {background_information}\",\n",
    "        input_variables=[\"background_information\"]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372e6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    "Software Engineer - Backend\n",
    "OpenAI\n",
    "\n",
    "We are looking for experienced backend engineers to join our team. Our ideal candidate will have experience with one or more of the following technologies: Python, Java, C++. \n",
    "\n",
    "Responsibilities:\n",
    "- Design and implement scalable and efficient backend systems\n",
    "- Write clean, maintainable code\n",
    "- Work with cross-functional teams\n",
    "\n",
    "Requirements:\n",
    "- Strong proficiency in one or more programming languages\n",
    "- Strong understanding of software design patterns and principles\n",
    "- Experience with distributed systems\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a27365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: \\n<Background>\\nSINCE THE USER IS APPPLYING FOR A JOB, THE QUERIES SHOULD BE WRITTEN IN A WAY THAT RESULST IN RELEVANT INFORMATION ABOUT THE COMPANY. THIS WILL HELP THE USER WRITE A MORE PERSONALIZED AND RELEVANT APPLICATION.\\n\\nCategory mapping (remember this!):\\n    query1 : recent developments\\n    query2 : recent news\\n    query3 : role-related info\\n    query4 : key customers & partners  \\n    query5 : culture & values\\n\\nNote: The above are just categories. The queries should be written in a way that results in relevant information about the company. Must include the company name in the query to ensure results have a higher confidence.\\n</Background>\\n\\n<Instructions>\\n    1. Each array must contain **exactly two** strings: [search_query, one_sentence_rationale]  \\n    2. If data is missing, craft a sensible fallback query; never return an empty array.  \\n    3. If the employer name cannot be found, use `\"UNKNOWN\"`.  \\n    4. Escape JSON only where required.\\n    5. Query cannot be repeated. It will lead to irrelevant results.\\n</Instructions>\\n\\n<EXAMPLE>\\n    JSON->\\n    \"query1\": (\"....\", \"...\")\\n    \"query2\": (\"....\", \"...\")\\n    \"query3\": (\"....\", \"...\")\\n    \"query4\": (\"....\", \"...\")\\n    \"query5\": (\"....\", \"...\")\\n</EXAMPLE>\\n                    \\nHuman: Below is the required job description and resume: {background_information}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_search_prompt.format(background_information=job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b973991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM with model llama3.2:latest and provider ollama in c:\\users\\risha\\python-dir\\knowledgebase\\job_writer\\utils\\llm_client.py\n",
      "Initializing LLM with model llama3.2:latest and provider ollama in c:\\users\\risha\\python-dir\\knowledgebase\\job_writer\\utils\\llm_client.py\n"
     ]
    }
   ],
   "source": [
    "from job_writer.utils.llm_client import LLMClient\n",
    "\n",
    "LLM = LLMClient()\n",
    "llm = LLMClient().get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5ac65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
